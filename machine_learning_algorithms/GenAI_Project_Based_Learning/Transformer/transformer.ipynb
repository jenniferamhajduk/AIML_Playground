{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers are complex. They have several parts that need to be implemented. Lets go step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This first part will be creating our embeddings. Remember again the lesson on data representation. We need to create these embeddings for the transformer to encode.\n",
    "\n",
    "### We need to turn the words into numbers. We will never forget data representation. Input will be words or word fragments, also known as \"tokens\". Each token goes through and activation function and multiplies it by a weight. We use the same embedding network for each phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dims):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dims)\n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding tells the transformer about the meaning and position of words in the input. We use a series of sine and cosine values. We use the y-values on the sine and cosine curves to find the corrosponding x-axis coordinate for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dims, max_seq_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Compute positional encodings for up to max_seq_len positions\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dims, 2) * -(math.log(10000.0) / embed_dims))\n",
    "        pos_enc = torch.zeros(max_seq_len, embed_dims)\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        pos_enc = pos_enc.unsqueeze(0)\n",
    "        \n",
    "        # Register the positional encodings as a buffer so they can be\n",
    "        # moved to the same device as the model's parameters\n",
    "        self.register_buffer('pos_enc', pos_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add the positional encodings to the input embeddings\n",
    "        x = x + self.pos_enc[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention works by comparing word similarity for every word in the sentence. In the transformer, we calculate the similarity between the query and keys. Larger similarities indicate stronger similarity. We then use a softmax function to determine what percentage of each word should be used to encode the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dims, nhead, dropout=0.1):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        \n",
    "        assert embed_dims % nhead == 0, \"embed_dims must be divisible by nhead\"\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = embed_dims // nhead\n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "        # Linear transformations for query, key, and value for each head\n",
    "        self.q_linear = nn.Linear(embed_dims, embed_dims)\n",
    "        self.k_linear = nn.Linear(embed_dims, embed_dims)\n",
    "        self.v_linear = nn.Linear(embed_dims, embed_dims)\n",
    "        \n",
    "        # Final linear transformation after concatenating the heads\n",
    "        self.out_linear = nn.Linear(embed_dims, embed_dims)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linearly transform query, key, and value for each head\n",
    "        query = self.q_linear(query).view(batch_size, -1, self.nhead, self.head_dim)\n",
    "        key = self.k_linear(key).view(batch_size, -1, self.nhead, self.head_dim)\n",
    "        value = self.v_linear(value).view(batch_size, -1, self.nhead, self.head_dim)\n",
    "        \n",
    "        # Transpose to make dimensions compatible for batch-wise matrix multiplication\n",
    "        query = query.permute(0, 2, 1, 3)\n",
    "        key = key.permute(0, 2, 1, 3)\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Compute scaled dot-product attention for each head\n",
    "        scale_factor = torch.sqrt(torch.tensor(self.head_dim, dtype=query.dtype))\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / scale_factor\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        # Reshape and concatenate the outputs from different heads\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.embed_dims)\n",
    "        \n",
    "        # Apply a final linear layer\n",
    "        output = self.out_linear(context)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
