{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code below utilizes the SKLearn library to simplify out processes in the previous example quite a bit. \n",
    "### We are able to get fit multiple documents to the BoW model, get our vocabulary, generate our document vectors and compare document similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'this': 33, 'is': 15, 'an': 0, 'example': 7, 'of': 20, 'using': 39, 'the': 31, 'sklearn': 29, 'tookit': 35, 'to': 34, 'produce': 22, 'vocabulary': 40, 'and': 1, 'output': 21, 'features': 9, 'each': 5, 'these': 32, 'sentences': 27, 'document': 3, 'will': 43, 'represent': 25, 'how': 13, 'we': 41, 'can': 2, 'have': 11, 'multiple': 19, 'documents': 4, 'shows': 28, 'much': 18, 'easier': 6, 'it': 16, 'get': 10, 'toolkit': 36, 'like': 17, 'hope': 12, 'see': 26, 'use': 38, 'existing': 8, 'toolkits': 37, 'rather': 23, 'than': 30, 're': 24, 'inventing': 14, 'wheel': 42}\n",
      "Document: This is an example of using the sklearn tookit to produce a vocabulary and output features.\n",
      "Vector: [1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0\n",
      " 0 0 1 1 0 0 0]\n",
      "\n",
      "Document: Each of these sentences is a document and each will represent how we can have multiple documents.\n",
      "Vector: [0 1 1 1 1 2 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 1]\n",
      "\n",
      "Document: This example shows how much easier it is to produce a vocabulary and get features using a toolkit like sklearn!\n",
      "Vector: [0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1\n",
      " 0 0 1 1 0 0 0]\n",
      "\n",
      "Document: I hope we see how much easier it is to use existing toolkits rather than re-inventing the wheel\n",
      "Vector: [0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
      " 1 1 0 0 1 1 0]\n",
      "\n",
      "Feature Names: ['an' 'and' 'can' 'document' 'documents' 'each' 'easier' 'example'\n",
      " 'existing' 'features' 'get' 'have' 'hope' 'how' 'inventing' 'is' 'it'\n",
      " 'like' 'much' 'multiple' 'of' 'output' 'produce' 'rather' 're'\n",
      " 'represent' 'see' 'sentences' 'shows' 'sklearn' 'than' 'the' 'these'\n",
      " 'this' 'to' 'tookit' 'toolkit' 'toolkits' 'use' 'using' 'vocabulary' 'we'\n",
      " 'wheel' 'will']\n",
      "Similarity between Document 1 and Document 2: 0.18257418583505536\n",
      "Similarity between Document 1 and Document 3: 0.6085806194501845\n",
      "Similarity between Document 1 and Document 4: 0.18257418583505536\n",
      "Similarity between Document 2 and Document 3: 0.1666666666666667\n",
      "Similarity between Document 2 and Document 4: 0.1666666666666667\n",
      "Similarity between Document 3 and Document 4: 0.3333333333333334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample documents - For the sake of brevity, each of these sentences is a \"document\"\n",
    "documents = [\n",
    "    \"This is an example of using the sklearn tookit to produce a vocabulary and output features.\",\n",
    "    \"Each of these sentences is a document and each will represent how we can have multiple documents.\",\n",
    "    \"This example shows how much easier it is to produce a vocabulary and get features using a toolkit like sklearn!\",\n",
    "    \"I hope we see how much easier it is to use existing toolkits rather than re-inventing the wheel\",\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform them into vectors - remember how we made vectors manually before? This is way easier\n",
    "document_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the vocabulary (feature names) from the vectorizer\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "\n",
    "# Convert the document vectors to a dense numpy array\n",
    "document_vectors = document_vectors.toarray()\n",
    "\n",
    "# Print the document vectors\n",
    "for i, document in enumerate(documents):\n",
    "    print(\"Document:\", document)\n",
    "    print(\"Vector:\", document_vectors[i])\n",
    "    print()\n",
    "\n",
    "# Get the feature names (words) from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the feature names\n",
    "print(\"Feature Names:\", feature_names)\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "cosine_similarities = cosine_similarity(document_vectors)\n",
    "\n",
    "# Print the pairwise cosine similarity\n",
    "num_documents = len(documents)\n",
    "for i in range(num_documents):\n",
    "    for j in range(i+1, num_documents):\n",
    "        similarity = cosine_similarities[i, j]\n",
    "        print(f\"Similarity between Document {i+1} and Document {j+1}: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
